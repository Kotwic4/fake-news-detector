{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, Sentence, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "import os\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../fakenewsnet_dataset'\n",
    "DATASET_NAME = 'politifact'\n",
    "DATASET_PATH = '{}/{}'.format(DATA_PATH, DATASET_NAME)\n",
    "REAL_DATA_PATH = '{}/real'.format(DATASET_PATH)\n",
    "FAKE_DATA_PATH = '{}/fake'.format(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_from_file(path):\n",
    "    with open(path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article():\n",
    "    def __init__(self, name, path):\n",
    "        self.path = path\n",
    "        self.name = name\n",
    "        self.content = None\n",
    "        self.tweets = []\n",
    "        \n",
    "    def load_content(self):\n",
    "        content_path = \"{}/news content.json\".format(self.path)\n",
    "        if os.path.isfile(content_path):\n",
    "            self.content = load_json_from_file(content_path)\n",
    "    \n",
    "    def load_tweets(self):\n",
    "        tweets_path = \"{}/tweets\".format(self.path)\n",
    "        if os.path.isdir(tweets_path):\n",
    "            tweets_files = os.listdir(tweets_path)\n",
    "            self.tweets = [load_json_from_file(\"{}/{}\".format(tweets_path, file)) for file in tweets_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_article(name, path):\n",
    "    art = Article(name, path)\n",
    "    art.load_content()\n",
    "    art.load_tweets()\n",
    "    return art\n",
    "\n",
    "def load_all_articles(path):\n",
    "    articles = []\n",
    "    if os.path.isdir(path):\n",
    "        articles_files = os.listdir(path)\n",
    "        articles = [load_single_article(file, \"{}/{}\".format(path, file)) for file in articles_files]\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_arts = load_all_articles(FAKE_DATA_PATH)\n",
    "real_arts = load_all_articles(REAL_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_arts_with_content = [art for art in fake_arts if art.content is not None]\n",
    "real_arts_with_content = [art for art in real_arts if art.content is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = [(art, 'fake') for art in fake_arts_with_content]\n",
    "real_data = [(art, 'real') for art in real_arts_with_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(fake_data)\n",
    "np.random.shuffle(real_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = fake_data[0:int(len(fake_data)*0.8)] + real_data[0:int(len(real_data)*0.8)]\n",
    "test_data = fake_data[int(len(fake_data)*0.8):] + real_data[int(len(real_data)*0.8):]\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643\n",
      "162\n",
      "805\n",
      "805\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(len(train_data) + len(test_data))\n",
    "print(len(fake_data) + len(real_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "class Classifier():\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def predict(self, text):\n",
    "        text = clear_text(text)\n",
    "        sentence = Sentence(text)\n",
    "        self.classifier.predict(sentence)\n",
    "        return sentence.labels[0]\n",
    "\n",
    "def transform_data(data):\n",
    "    return [{'label': label, 'text': clear_text(x)} for x, label in data]\n",
    "\n",
    "def save_data(data, data_folder = '.'):\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    data = transform_data(data)\n",
    "    frame_data = pd.DataFrame(data)\n",
    "    train_path = '{}/train.csv'.format(data_folder)\n",
    "    test_path = '{}/test.csv'.format(data_folder)\n",
    "    dev_path = '{}/dev.csv'.format(data_folder)\n",
    "    frame_data.iloc[0:int(len(data)*0.8)].to_csv(train_path, sep='\\t', index = False, header = False)\n",
    "    frame_data.iloc[int(len(data)*0.8):int(len(data)*0.9)].to_csv(test_path, sep='\\t', index = False, header = False)\n",
    "    frame_data.iloc[int(len(data)*0.9):].to_csv(dev_path, sep='\\t', index = False, header = False)\n",
    "\n",
    "def load_corpus(data_folder = '.'):\n",
    "    column_name_map = {1: \"text\", 0: \"label\"}\n",
    "    return CSVClassificationCorpus(data_folder,\n",
    "                                     column_name_map,\n",
    "                                     delimiter='\\t',\n",
    "                                  test_file='test.csv',\n",
    "                                  dev_file='dev.csv',\n",
    "                                  train_file='train.csv')\n",
    "    \n",
    "def train_classifier(corpus, model_folder = '.', max_epochs = 1):\n",
    "    label_dict = corpus.make_label_dictionary()\n",
    "\n",
    "    word_embeddings = [\n",
    "        WordEmbeddings('glove'),\n",
    "        FlairEmbeddings('news-forward-fast'),\n",
    "        FlairEmbeddings('news-backward-fast')\n",
    "    ]\n",
    "\n",
    "    document_embeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                hidden_size=512,\n",
    "                                                reproject_words=True,\n",
    "                                                reproject_words_dimension=256)\n",
    "\n",
    "    classifier = TextClassifier(document_embeddings,\n",
    "                                label_dictionary=label_dict)\n",
    "\n",
    "    trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "    trainer.train(model_folder, max_epochs=max_epochs)\n",
    "    \n",
    "    return TextClassifier.load('{}/best-model.pt'.format(model_folder))\n",
    "    \n",
    "def train_model(train_data,\n",
    "               data_folder = '.',\n",
    "               model_folder = '.',\n",
    "               max_epochs=1\n",
    "               ):\n",
    "    save_data(train_data, data_folder)\n",
    "    corpus = load_corpus(data_folder)\n",
    "    classifier = train_classifier(corpus, model_folder, max_epochs)\n",
    "    return Classifier(classifier)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, pos_label = 'fake'):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label=pos_label)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=pos_label)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=pos_label)\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "def validate_model(test_data, classifier):\n",
    "    y_true = [label for x, label in test_data]\n",
    "    y_pred = [classifier.predict(x).value for x, label in test_data]\n",
    "    acc, precision, recall, f1 = calculate_metrics(y_true, y_pred)\n",
    "    print(\"acc: \", acc)\n",
    "    print(\"precision: \", precision)\n",
    "    print(\"recall: \", recall)\n",
    "    print(\"f1: \", f1)\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "def make_test(train_data, test_data, data_folder, model_folder, max_epochs):\n",
    "    classifier = train_model(train_data, data_folder, model_folder, max_epochs)\n",
    "    validate_model(test_data, classifier)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content = [(x.content, label) for x, label in train_data] \n",
    "test_content = [(x.content, label) for x, label in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "6\n",
      "608\n",
      "156\n"
     ]
    }
   ],
   "source": [
    "train_title = [(x['title'], label) for x, label in train_content] \n",
    "test_title = [(x['title'], label) for x, label in test_content]\n",
    "print(len([x for x, label in train_title if x == '']))\n",
    "print(len([x for x, label in test_title if x == '']))\n",
    "train_title = [(x, label) for x, label in train_title if x != ''] \n",
    "test_title = [(x, label) for x, label in test_title if x != '']\n",
    "print(len(train_title))\n",
    "print(len(test_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:28:52,755 Reading data from test/title1\n",
      "2019-11-23 23:28:52,756 Train: test/title1/train.csv\n",
      "2019-11-23 23:28:52,757 Dev: test/title1/dev.csv\n",
      "2019-11-23 23:28:52,760 Test: test/title1/test.csv\n",
      "2019-11-23 23:28:52,767 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 486/486 [00:00<00:00, 486.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:28:55,276 [b'real', b'fake']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:28:56,730 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:28:56,731 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2148, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-11-23 23:28:56,732 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:28:56,734 Corpus: \"Corpus: 486 train + 61 dev + 61 test sentences\"\n",
      "2019-11-23 23:28:56,734 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:28:56,735 Parameters:\n",
      "2019-11-23 23:28:56,736  - learning_rate: \"0.1\"\n",
      "2019-11-23 23:28:56,738  - mini_batch_size: \"32\"\n",
      "2019-11-23 23:28:56,739  - patience: \"3\"\n",
      "2019-11-23 23:28:56,739  - anneal_factor: \"0.5\"\n",
      "2019-11-23 23:28:56,742  - max_epochs: \"1\"\n",
      "2019-11-23 23:28:56,744  - shuffle: \"True\"\n",
      "2019-11-23 23:28:56,745  - train_with_dev: \"False\"\n",
      "2019-11-23 23:28:56,746  - batch_growth_annealing: \"False\"\n",
      "2019-11-23 23:28:56,747 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:28:56,749 Model training base path: \"test/title1\"\n",
      "2019-11-23 23:28:56,750 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:28:56,752 Device: cpu\n",
      "2019-11-23 23:28:56,753 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:28:56,754 Embeddings storage mode: cpu\n",
      "2019-11-23 23:28:56,757 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:29:00,480 epoch 1 - iter 0/16 - loss 0.70934469 - samples/sec: 13.45\n",
      "2019-11-23 23:29:03,137 epoch 1 - iter 1/16 - loss 0.71176985 - samples/sec: 12.17\n",
      "2019-11-23 23:29:05,526 epoch 1 - iter 2/16 - loss 0.67189548 - samples/sec: 13.47\n",
      "2019-11-23 23:29:07,970 epoch 1 - iter 3/16 - loss 0.71113332 - samples/sec: 13.21\n",
      "2019-11-23 23:29:10,145 epoch 1 - iter 4/16 - loss 0.69373806 - samples/sec: 14.84\n",
      "2019-11-23 23:29:12,662 epoch 1 - iter 5/16 - loss 0.68724857 - samples/sec: 12.98\n",
      "2019-11-23 23:29:14,721 epoch 1 - iter 6/16 - loss 0.68457099 - samples/sec: 15.77\n",
      "2019-11-23 23:29:17,096 epoch 1 - iter 7/16 - loss 0.67551368 - samples/sec: 13.65\n",
      "2019-11-23 23:29:18,541 epoch 1 - iter 8/16 - loss 0.67429148 - samples/sec: 22.36\n",
      "2019-11-23 23:29:20,959 epoch 1 - iter 9/16 - loss 0.68484542 - samples/sec: 13.32\n",
      "2019-11-23 23:29:23,613 epoch 1 - iter 10/16 - loss 0.67429988 - samples/sec: 12.14\n",
      "2019-11-23 23:29:26,054 epoch 1 - iter 11/16 - loss 0.67117391 - samples/sec: 13.28\n",
      "2019-11-23 23:29:27,935 epoch 1 - iter 12/16 - loss 0.66418704 - samples/sec: 17.17\n",
      "2019-11-23 23:29:29,875 epoch 1 - iter 13/16 - loss 0.65874728 - samples/sec: 16.67\n",
      "2019-11-23 23:29:31,530 epoch 1 - iter 14/16 - loss 0.65243059 - samples/sec: 19.52\n",
      "2019-11-23 23:29:32,376 epoch 1 - iter 15/16 - loss 0.65577308 - samples/sec: 38.40\n",
      "2019-11-23 23:29:32,817 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:29:32,818 EPOCH 1 done: loss 0.6558 - lr 0.1000\n",
      "2019-11-23 23:29:37,983 DEV : loss 0.5986052751541138 - score 0.6393\n",
      "2019-11-23 23:29:38,036 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:29:46,025 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:29:46,026 Testing using best model ...\n",
      "2019-11-23 23:29:46,028 loading file test/title1/best-model.pt\n",
      "2019-11-23 23:30:12,689 0.7869\t0.7869\t0.7869\n",
      "2019-11-23 23:30:12,695 \n",
      "MICRO_AVG: acc 0.6486 - f1-score 0.7869\n",
      "MACRO_AVG: acc 0.6477 - f1-score 0.786\n",
      "fake       tp: 26 - fp: 11 - fn: 2 - tn: 22 - precision: 0.7027 - recall: 0.9286 - accuracy: 0.6667 - f1-score: 0.8000\n",
      "real       tp: 22 - fp: 2 - fn: 11 - tn: 26 - precision: 0.9167 - recall: 0.6667 - accuracy: 0.6286 - f1-score: 0.7720\n",
      "2019-11-23 23:30:12,696 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:30:12,699 loading file ./test/title1/best-model.pt\n",
      "acc:  0.6923076923076923\n",
      "precision:  0.6355140186915887\n",
      "recall:  0.8831168831168831\n",
      "f1:  0.7391304347826086\n"
     ]
    }
   ],
   "source": [
    "path = './test/title1'\n",
    "make_test(train_title, test_title, path, path, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:32:31,004 Reading data from test/title10\n",
      "2019-11-23 23:32:31,005 Train: test/title10/train.csv\n",
      "2019-11-23 23:32:31,006 Dev: test/title10/dev.csv\n",
      "2019-11-23 23:32:31,007 Test: test/title10/test.csv\n",
      "2019-11-23 23:32:31,012 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 486/486 [00:00<00:00, 520.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:32:33,404 [b'real', b'fake']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:32:34,863 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:32:34,864 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "      (list_embedding_1): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (list_embedding_2): FlairEmbeddings(\n",
      "        (lm): LanguageModel(\n",
      "          (drop): Dropout(p=0.25, inplace=False)\n",
      "          (encoder): Embedding(275, 100)\n",
      "          (rnn): LSTM(100, 1024)\n",
      "          (decoder): Linear(in_features=1024, out_features=275, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=2148, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\"\n",
      "2019-11-23 23:32:34,865 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:32:34,866 Corpus: \"Corpus: 486 train + 61 dev + 61 test sentences\"\n",
      "2019-11-23 23:32:34,867 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:32:34,867 Parameters:\n",
      "2019-11-23 23:32:34,868  - learning_rate: \"0.1\"\n",
      "2019-11-23 23:32:34,869  - mini_batch_size: \"32\"\n",
      "2019-11-23 23:32:34,870  - patience: \"3\"\n",
      "2019-11-23 23:32:34,871  - anneal_factor: \"0.5\"\n",
      "2019-11-23 23:32:34,872  - max_epochs: \"10\"\n",
      "2019-11-23 23:32:34,872  - shuffle: \"True\"\n",
      "2019-11-23 23:32:34,873  - train_with_dev: \"False\"\n",
      "2019-11-23 23:32:34,874  - batch_growth_annealing: \"False\"\n",
      "2019-11-23 23:32:34,876 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:32:34,879 Model training base path: \"test/title10\"\n",
      "2019-11-23 23:32:34,880 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:32:34,883 Device: cpu\n",
      "2019-11-23 23:32:34,885 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:32:34,886 Embeddings storage mode: cpu\n",
      "2019-11-23 23:32:34,888 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:32:38,987 epoch 1 - iter 0/16 - loss 0.69027871 - samples/sec: 11.09\n",
      "2019-11-23 23:32:41,862 epoch 1 - iter 1/16 - loss 0.69057435 - samples/sec: 11.57\n",
      "2019-11-23 23:32:43,662 epoch 1 - iter 2/16 - loss 0.69871442 - samples/sec: 17.90\n",
      "2019-11-23 23:32:46,087 epoch 1 - iter 3/16 - loss 0.69128485 - samples/sec: 13.28\n",
      "2019-11-23 23:32:47,592 epoch 1 - iter 4/16 - loss 0.67877431 - samples/sec: 21.48\n",
      "2019-11-23 23:32:50,816 epoch 1 - iter 5/16 - loss 0.66971946 - samples/sec: 9.96\n",
      "2019-11-23 23:32:52,733 epoch 1 - iter 6/16 - loss 0.66559554 - samples/sec: 16.99\n",
      "2019-11-23 23:32:53,928 epoch 1 - iter 7/16 - loss 0.67223248 - samples/sec: 27.12\n",
      "2019-11-23 23:32:56,139 epoch 1 - iter 8/16 - loss 0.67505061 - samples/sec: 14.57\n",
      "2019-11-23 23:32:57,873 epoch 1 - iter 9/16 - loss 0.67646434 - samples/sec: 18.62\n",
      "2019-11-23 23:32:59,197 epoch 1 - iter 10/16 - loss 0.67238332 - samples/sec: 24.45\n",
      "2019-11-23 23:33:00,521 epoch 1 - iter 11/16 - loss 0.67017030 - samples/sec: 24.50\n",
      "2019-11-23 23:33:01,998 epoch 1 - iter 12/16 - loss 0.67306008 - samples/sec: 22.39\n",
      "2019-11-23 23:33:04,404 epoch 1 - iter 13/16 - loss 0.66355071 - samples/sec: 13.37\n",
      "2019-11-23 23:33:06,747 epoch 1 - iter 14/16 - loss 0.65838774 - samples/sec: 13.75\n",
      "2019-11-23 23:33:07,627 epoch 1 - iter 15/16 - loss 0.64145711 - samples/sec: 36.91\n",
      "2019-11-23 23:33:08,476 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:33:08,477 EPOCH 1 done: loss 0.6415 - lr 0.1000\n",
      "2019-11-23 23:33:13,746 DEV : loss 0.7614424228668213 - score 0.5246\n",
      "2019-11-23 23:33:13,805 BAD EPOCHS (no improvement): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DocumentRNNEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type StackedEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type WordEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:33:17,329 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type FlairEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LanguageModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/kotwic4/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:33:20,947 epoch 2 - iter 0/16 - loss 0.51937228 - samples/sec: 12.71\n",
      "2019-11-23 23:33:23,339 epoch 2 - iter 1/16 - loss 0.54447788 - samples/sec: 13.54\n",
      "2019-11-23 23:33:25,443 epoch 2 - iter 2/16 - loss 0.55576207 - samples/sec: 15.35\n",
      "2019-11-23 23:33:27,783 epoch 2 - iter 3/16 - loss 0.56155623 - samples/sec: 13.76\n",
      "2019-11-23 23:33:30,042 epoch 2 - iter 4/16 - loss 0.55056062 - samples/sec: 14.36\n",
      "2019-11-23 23:33:31,484 epoch 2 - iter 5/16 - loss 0.54723892 - samples/sec: 22.43\n",
      "2019-11-23 23:33:33,931 epoch 2 - iter 6/16 - loss 0.56366252 - samples/sec: 13.18\n",
      "2019-11-23 23:33:35,435 epoch 2 - iter 7/16 - loss 0.56681088 - samples/sec: 21.64\n",
      "2019-11-23 23:33:36,831 epoch 2 - iter 8/16 - loss 0.56928225 - samples/sec: 23.19\n",
      "2019-11-23 23:33:38,448 epoch 2 - iter 9/16 - loss 0.56456451 - samples/sec: 19.94\n",
      "2019-11-23 23:33:39,876 epoch 2 - iter 10/16 - loss 0.56214343 - samples/sec: 22.88\n",
      "2019-11-23 23:33:41,119 epoch 2 - iter 11/16 - loss 0.55706996 - samples/sec: 26.02\n",
      "2019-11-23 23:33:42,495 epoch 2 - iter 12/16 - loss 0.54753752 - samples/sec: 23.53\n",
      "2019-11-23 23:33:44,805 epoch 2 - iter 13/16 - loss 0.54304906 - samples/sec: 13.97\n",
      "2019-11-23 23:33:47,229 epoch 2 - iter 14/16 - loss 0.53361718 - samples/sec: 13.34\n",
      "2019-11-23 23:33:48,190 epoch 2 - iter 15/16 - loss 0.54149941 - samples/sec: 33.75\n",
      "2019-11-23 23:33:48,768 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:33:48,769 EPOCH 2 done: loss 0.5415 - lr 0.1000\n",
      "2019-11-23 23:33:53,684 DEV : loss 0.7565724849700928 - score 0.5574\n",
      "2019-11-23 23:33:53,734 BAD EPOCHS (no improvement): 0\n",
      "2019-11-23 23:33:57,643 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:34:01,179 epoch 3 - iter 0/16 - loss 0.58910710 - samples/sec: 13.02\n",
      "2019-11-23 23:34:02,472 epoch 3 - iter 1/16 - loss 0.52205834 - samples/sec: 25.11\n",
      "2019-11-23 23:34:03,978 epoch 3 - iter 2/16 - loss 0.49285199 - samples/sec: 21.43\n",
      "2019-11-23 23:34:05,690 epoch 3 - iter 3/16 - loss 0.48414227 - samples/sec: 18.84\n",
      "2019-11-23 23:34:06,908 epoch 3 - iter 4/16 - loss 0.48361430 - samples/sec: 26.63\n",
      "2019-11-23 23:34:09,311 epoch 3 - iter 5/16 - loss 0.46860431 - samples/sec: 13.39\n",
      "2019-11-23 23:34:10,805 epoch 3 - iter 6/16 - loss 0.45155177 - samples/sec: 21.81\n",
      "2019-11-23 23:34:12,281 epoch 3 - iter 7/16 - loss 0.45179882 - samples/sec: 21.89\n",
      "2019-11-23 23:34:14,719 epoch 3 - iter 8/16 - loss 0.45634331 - samples/sec: 13.23\n",
      "2019-11-23 23:34:17,293 epoch 3 - iter 9/16 - loss 0.47184667 - samples/sec: 12.55\n",
      "2019-11-23 23:34:18,395 epoch 3 - iter 10/16 - loss 0.49264963 - samples/sec: 29.41\n",
      "2019-11-23 23:34:20,845 epoch 3 - iter 11/16 - loss 0.49738999 - samples/sec: 13.14\n",
      "2019-11-23 23:34:22,297 epoch 3 - iter 12/16 - loss 0.49402943 - samples/sec: 22.45\n",
      "2019-11-23 23:34:23,654 epoch 3 - iter 13/16 - loss 0.51006005 - samples/sec: 23.84\n",
      "2019-11-23 23:34:25,931 epoch 3 - iter 14/16 - loss 0.51871527 - samples/sec: 14.16\n",
      "2019-11-23 23:34:27,586 epoch 3 - iter 15/16 - loss 0.50860136 - samples/sec: 19.55\n",
      "2019-11-23 23:34:28,023 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:34:28,024 EPOCH 3 done: loss 0.5086 - lr 0.1000\n",
      "2019-11-23 23:34:33,103 DEV : loss 0.43999430537223816 - score 0.8033\n",
      "2019-11-23 23:34:33,142 BAD EPOCHS (no improvement): 0\n",
      "2019-11-23 23:34:37,032 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:34:39,636 epoch 4 - iter 0/16 - loss 0.41568914 - samples/sec: 21.84\n",
      "2019-11-23 23:34:41,985 epoch 4 - iter 1/16 - loss 0.40508640 - samples/sec: 13.83\n",
      "2019-11-23 23:34:44,118 epoch 4 - iter 2/16 - loss 0.45198903 - samples/sec: 15.10\n",
      "2019-11-23 23:34:45,452 epoch 4 - iter 3/16 - loss 0.44718857 - samples/sec: 24.25\n",
      "2019-11-23 23:34:46,947 epoch 4 - iter 4/16 - loss 0.53081172 - samples/sec: 21.58\n",
      "2019-11-23 23:34:49,005 epoch 4 - iter 5/16 - loss 0.55393116 - samples/sec: 15.77\n",
      "2019-11-23 23:34:51,540 epoch 4 - iter 6/16 - loss 0.53540536 - samples/sec: 12.70\n",
      "2019-11-23 23:34:54,350 epoch 4 - iter 7/16 - loss 0.53314300 - samples/sec: 11.52\n",
      "2019-11-23 23:34:55,705 epoch 4 - iter 8/16 - loss 0.53861359 - samples/sec: 23.88\n",
      "2019-11-23 23:34:58,678 epoch 4 - iter 9/16 - loss 0.52621543 - samples/sec: 10.82\n",
      "2019-11-23 23:35:00,858 epoch 4 - iter 10/16 - loss 0.52232849 - samples/sec: 14.79\n",
      "2019-11-23 23:35:03,845 epoch 4 - iter 11/16 - loss 0.52795573 - samples/sec: 10.82\n",
      "2019-11-23 23:35:05,495 epoch 4 - iter 12/16 - loss 0.52363920 - samples/sec: 19.59\n",
      "2019-11-23 23:35:07,526 epoch 4 - iter 13/16 - loss 0.52114675 - samples/sec: 15.94\n",
      "2019-11-23 23:35:11,539 epoch 4 - iter 14/16 - loss 0.51520513 - samples/sec: 8.04\n",
      "2019-11-23 23:35:13,201 epoch 4 - iter 15/16 - loss 0.52460286 - samples/sec: 19.41\n",
      "2019-11-23 23:35:14,040 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:35:14,041 EPOCH 4 done: loss 0.5246 - lr 0.1000\n",
      "2019-11-23 23:35:21,840 DEV : loss 0.422730952501297 - score 0.7541\n",
      "2019-11-23 23:35:21,908 BAD EPOCHS (no improvement): 1\n",
      "2019-11-23 23:35:21,910 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:35:26,215 epoch 5 - iter 0/16 - loss 0.44911611 - samples/sec: 11.99\n",
      "2019-11-23 23:35:27,693 epoch 5 - iter 1/16 - loss 0.51620194 - samples/sec: 21.93\n",
      "2019-11-23 23:35:29,720 epoch 5 - iter 2/16 - loss 0.52927574 - samples/sec: 15.89\n",
      "2019-11-23 23:35:32,163 epoch 5 - iter 3/16 - loss 0.48782811 - samples/sec: 13.20\n",
      "2019-11-23 23:35:34,010 epoch 5 - iter 4/16 - loss 0.45222225 - samples/sec: 17.47\n",
      "2019-11-23 23:35:36,699 epoch 5 - iter 5/16 - loss 0.46611300 - samples/sec: 12.02\n",
      "2019-11-23 23:35:38,198 epoch 5 - iter 6/16 - loss 0.47036405 - samples/sec: 21.64\n",
      "2019-11-23 23:35:39,927 epoch 5 - iter 7/16 - loss 0.47783964 - samples/sec: 18.70\n",
      "2019-11-23 23:35:41,528 epoch 5 - iter 8/16 - loss 0.49453749 - samples/sec: 20.28\n",
      "2019-11-23 23:35:44,087 epoch 5 - iter 9/16 - loss 0.47638718 - samples/sec: 12.57\n",
      "2019-11-23 23:35:47,528 epoch 5 - iter 10/16 - loss 0.47992192 - samples/sec: 9.34\n",
      "2019-11-23 23:35:50,729 epoch 5 - iter 11/16 - loss 0.47056871 - samples/sec: 10.08\n",
      "2019-11-23 23:35:52,386 epoch 5 - iter 12/16 - loss 0.46474889 - samples/sec: 19.47\n",
      "2019-11-23 23:35:54,904 epoch 5 - iter 13/16 - loss 0.45382324 - samples/sec: 12.84\n",
      "2019-11-23 23:35:56,461 epoch 5 - iter 14/16 - loss 0.45786064 - samples/sec: 20.81\n",
      "2019-11-23 23:35:57,049 epoch 5 - iter 15/16 - loss 0.44923671 - samples/sec: 55.95\n",
      "2019-11-23 23:35:57,634 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:35:57,636 EPOCH 5 done: loss 0.4492 - lr 0.1000\n",
      "2019-11-23 23:36:03,735 DEV : loss 0.38555586338043213 - score 0.8197\n",
      "2019-11-23 23:36:03,812 BAD EPOCHS (no improvement): 0\n",
      "2019-11-23 23:36:08,739 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:36:13,239 epoch 6 - iter 0/16 - loss 0.55871290 - samples/sec: 10.10\n",
      "2019-11-23 23:36:15,601 epoch 6 - iter 1/16 - loss 0.41161522 - samples/sec: 13.74\n",
      "2019-11-23 23:36:19,096 epoch 6 - iter 2/16 - loss 0.37464242 - samples/sec: 9.22\n",
      "2019-11-23 23:36:21,066 epoch 6 - iter 3/16 - loss 0.36852967 - samples/sec: 16.38\n",
      "2019-11-23 23:36:22,966 epoch 6 - iter 4/16 - loss 0.39049849 - samples/sec: 17.04\n",
      "2019-11-23 23:36:25,629 epoch 6 - iter 5/16 - loss 0.39258046 - samples/sec: 12.08\n",
      "2019-11-23 23:36:26,893 epoch 6 - iter 6/16 - loss 0.39894300 - samples/sec: 25.60\n",
      "2019-11-23 23:36:28,426 epoch 6 - iter 7/16 - loss 0.40644821 - samples/sec: 21.07\n",
      "2019-11-23 23:36:30,583 epoch 6 - iter 8/16 - loss 0.41399178 - samples/sec: 15.08\n",
      "2019-11-23 23:36:33,734 epoch 6 - iter 9/16 - loss 0.43267909 - samples/sec: 10.21\n",
      "2019-11-23 23:36:35,864 epoch 6 - iter 10/16 - loss 0.42624634 - samples/sec: 15.26\n",
      "2019-11-23 23:36:38,641 epoch 6 - iter 11/16 - loss 0.41468292 - samples/sec: 11.58\n",
      "2019-11-23 23:36:41,666 epoch 6 - iter 12/16 - loss 0.42490591 - samples/sec: 10.63\n",
      "2019-11-23 23:36:43,665 epoch 6 - iter 13/16 - loss 0.41658789 - samples/sec: 16.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:36:46,251 epoch 6 - iter 14/16 - loss 0.41754213 - samples/sec: 12.48\n",
      "2019-11-23 23:36:46,987 epoch 6 - iter 15/16 - loss 0.40156722 - samples/sec: 44.35\n",
      "2019-11-23 23:36:47,568 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:36:47,570 EPOCH 6 done: loss 0.4016 - lr 0.1000\n",
      "2019-11-23 23:36:54,343 DEV : loss 0.3670271039009094 - score 0.8197\n",
      "2019-11-23 23:36:54,393 BAD EPOCHS (no improvement): 1\n",
      "2019-11-23 23:36:58,985 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:37:02,392 epoch 7 - iter 0/16 - loss 0.29282936 - samples/sec: 15.52\n",
      "2019-11-23 23:37:04,414 epoch 7 - iter 1/16 - loss 0.35789925 - samples/sec: 16.25\n",
      "2019-11-23 23:37:06,515 epoch 7 - iter 2/16 - loss 0.39505283 - samples/sec: 15.34\n",
      "2019-11-23 23:37:08,205 epoch 7 - iter 3/16 - loss 0.37502851 - samples/sec: 19.15\n",
      "2019-11-23 23:37:12,020 epoch 7 - iter 4/16 - loss 0.39226651 - samples/sec: 8.43\n",
      "2019-11-23 23:37:13,494 epoch 7 - iter 5/16 - loss 0.38801597 - samples/sec: 22.00\n",
      "2019-11-23 23:37:16,723 epoch 7 - iter 6/16 - loss 0.41906738 - samples/sec: 10.00\n",
      "2019-11-23 23:37:19,132 epoch 7 - iter 7/16 - loss 0.41896587 - samples/sec: 13.43\n",
      "2019-11-23 23:37:22,098 epoch 7 - iter 8/16 - loss 0.44602910 - samples/sec: 10.86\n",
      "2019-11-23 23:37:25,357 epoch 7 - iter 9/16 - loss 0.44485950 - samples/sec: 9.88\n",
      "2019-11-23 23:37:28,276 epoch 7 - iter 10/16 - loss 0.44664585 - samples/sec: 11.04\n",
      "2019-11-23 23:37:30,749 epoch 7 - iter 11/16 - loss 0.44392618 - samples/sec: 13.04\n",
      "2019-11-23 23:37:32,411 epoch 7 - iter 12/16 - loss 0.43614258 - samples/sec: 19.55\n",
      "2019-11-23 23:37:34,083 epoch 7 - iter 13/16 - loss 0.44015986 - samples/sec: 19.32\n",
      "2019-11-23 23:37:36,708 epoch 7 - iter 14/16 - loss 0.43761076 - samples/sec: 12.26\n",
      "2019-11-23 23:37:37,454 epoch 7 - iter 15/16 - loss 0.44984121 - samples/sec: 43.85\n",
      "2019-11-23 23:37:38,144 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:37:38,145 EPOCH 7 done: loss 0.4498 - lr 0.1000\n",
      "2019-11-23 23:37:44,872 DEV : loss 0.8899275064468384 - score 0.541\n",
      "2019-11-23 23:37:44,924 BAD EPOCHS (no improvement): 2\n",
      "2019-11-23 23:37:44,926 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:37:48,920 epoch 8 - iter 0/16 - loss 0.87704241 - samples/sec: 14.40\n",
      "2019-11-23 23:37:51,434 epoch 8 - iter 1/16 - loss 0.61344188 - samples/sec: 12.86\n",
      "2019-11-23 23:37:53,209 epoch 8 - iter 2/16 - loss 0.58427763 - samples/sec: 18.21\n",
      "2019-11-23 23:37:56,706 epoch 8 - iter 3/16 - loss 0.52559652 - samples/sec: 9.22\n",
      "2019-11-23 23:37:58,474 epoch 8 - iter 4/16 - loss 0.49156322 - samples/sec: 18.28\n",
      "2019-11-23 23:38:01,806 epoch 8 - iter 5/16 - loss 0.47715517 - samples/sec: 9.69\n",
      "2019-11-23 23:38:03,721 epoch 8 - iter 6/16 - loss 0.48591294 - samples/sec: 16.89\n",
      "2019-11-23 23:38:06,261 epoch 8 - iter 7/16 - loss 0.48578027 - samples/sec: 12.68\n",
      "2019-11-23 23:38:08,910 epoch 8 - iter 8/16 - loss 0.46621877 - samples/sec: 12.16\n",
      "2019-11-23 23:38:10,636 epoch 8 - iter 9/16 - loss 0.45423167 - samples/sec: 18.69\n",
      "2019-11-23 23:38:13,113 epoch 8 - iter 10/16 - loss 0.43953464 - samples/sec: 12.99\n",
      "2019-11-23 23:38:14,791 epoch 8 - iter 11/16 - loss 0.43309120 - samples/sec: 19.35\n",
      "2019-11-23 23:38:16,388 epoch 8 - iter 12/16 - loss 0.44022048 - samples/sec: 20.27\n",
      "2019-11-23 23:38:19,491 epoch 8 - iter 13/16 - loss 0.43117179 - samples/sec: 10.38\n",
      "2019-11-23 23:38:22,146 epoch 8 - iter 14/16 - loss 0.42088858 - samples/sec: 12.14\n",
      "2019-11-23 23:38:23,027 epoch 8 - iter 15/16 - loss 0.42806152 - samples/sec: 36.93\n",
      "2019-11-23 23:38:23,799 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:38:23,801 EPOCH 8 done: loss 0.4281 - lr 0.1000\n",
      "2019-11-23 23:38:30,390 DEV : loss 1.2806551456451416 - score 0.5082\n",
      "2019-11-23 23:38:30,428 BAD EPOCHS (no improvement): 3\n",
      "2019-11-23 23:38:30,430 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:38:33,245 epoch 9 - iter 0/16 - loss 1.08132768 - samples/sec: 22.67\n",
      "2019-11-23 23:38:35,900 epoch 9 - iter 1/16 - loss 0.72938223 - samples/sec: 12.13\n",
      "2019-11-23 23:38:37,554 epoch 9 - iter 2/16 - loss 0.60467163 - samples/sec: 19.52\n",
      "2019-11-23 23:38:39,056 epoch 9 - iter 3/16 - loss 0.53111647 - samples/sec: 21.72\n",
      "2019-11-23 23:38:41,069 epoch 9 - iter 4/16 - loss 0.48869313 - samples/sec: 16.07\n",
      "2019-11-23 23:38:44,255 epoch 9 - iter 5/16 - loss 0.46679470 - samples/sec: 10.14\n",
      "2019-11-23 23:38:45,753 epoch 9 - iter 6/16 - loss 0.45270222 - samples/sec: 21.61\n",
      "2019-11-23 23:38:47,484 epoch 9 - iter 7/16 - loss 0.45211869 - samples/sec: 18.70\n",
      "2019-11-23 23:38:49,843 epoch 9 - iter 8/16 - loss 0.43598603 - samples/sec: 13.65\n",
      "2019-11-23 23:38:52,691 epoch 9 - iter 9/16 - loss 0.44613662 - samples/sec: 11.34\n",
      "2019-11-23 23:38:55,894 epoch 9 - iter 10/16 - loss 0.43864144 - samples/sec: 10.10\n",
      "2019-11-23 23:38:58,654 epoch 9 - iter 11/16 - loss 0.43194967 - samples/sec: 11.72\n",
      "2019-11-23 23:39:00,034 epoch 9 - iter 12/16 - loss 0.41674312 - samples/sec: 23.58\n",
      "2019-11-23 23:39:01,308 epoch 9 - iter 13/16 - loss 0.40595510 - samples/sec: 25.48\n",
      "2019-11-23 23:39:04,023 epoch 9 - iter 14/16 - loss 0.39814256 - samples/sec: 11.84\n",
      "2019-11-23 23:39:05,489 epoch 9 - iter 15/16 - loss 0.40606098 - samples/sec: 22.06\n",
      "2019-11-23 23:39:06,473 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:39:06,475 EPOCH 9 done: loss 0.4061 - lr 0.1000\n",
      "2019-11-23 23:39:12,849 DEV : loss 0.3341987431049347 - score 0.8197\n",
      "Epoch     8: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2019-11-23 23:39:12,893 BAD EPOCHS (no improvement): 4\n",
      "2019-11-23 23:39:16,866 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:39:20,521 epoch 10 - iter 0/16 - loss 0.28891963 - samples/sec: 14.30\n",
      "2019-11-23 23:39:22,591 epoch 10 - iter 1/16 - loss 0.31699502 - samples/sec: 15.85\n",
      "2019-11-23 23:39:24,509 epoch 10 - iter 2/16 - loss 0.29954935 - samples/sec: 16.85\n",
      "2019-11-23 23:39:27,272 epoch 10 - iter 3/16 - loss 0.29175363 - samples/sec: 11.67\n",
      "2019-11-23 23:39:29,747 epoch 10 - iter 4/16 - loss 0.36718706 - samples/sec: 13.02\n",
      "2019-11-23 23:39:32,530 epoch 10 - iter 5/16 - loss 0.35838704 - samples/sec: 11.56\n",
      "2019-11-23 23:39:34,239 epoch 10 - iter 6/16 - loss 0.33831938 - samples/sec: 18.93\n",
      "2019-11-23 23:39:37,166 epoch 10 - iter 7/16 - loss 0.34627991 - samples/sec: 11.04\n",
      "2019-11-23 23:39:39,056 epoch 10 - iter 8/16 - loss 0.35304649 - samples/sec: 17.05\n",
      "2019-11-23 23:39:40,590 epoch 10 - iter 9/16 - loss 0.37380866 - samples/sec: 21.15\n",
      "2019-11-23 23:39:43,625 epoch 10 - iter 10/16 - loss 0.36833270 - samples/sec: 10.59\n",
      "2019-11-23 23:39:45,341 epoch 10 - iter 11/16 - loss 0.36474671 - samples/sec: 18.83\n",
      "2019-11-23 23:39:46,834 epoch 10 - iter 12/16 - loss 0.35989548 - samples/sec: 21.72\n",
      "2019-11-23 23:39:49,330 epoch 10 - iter 13/16 - loss 0.37056017 - samples/sec: 12.96\n",
      "2019-11-23 23:39:52,204 epoch 10 - iter 14/16 - loss 0.36451801 - samples/sec: 11.18\n",
      "2019-11-23 23:39:53,688 epoch 10 - iter 15/16 - loss 0.35153859 - samples/sec: 21.76\n",
      "2019-11-23 23:39:54,531 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:39:54,532 EPOCH 10 done: loss 0.3515 - lr 0.0500\n",
      "2019-11-23 23:40:00,905 DEV : loss 0.3844107985496521 - score 0.7869\n",
      "2019-11-23 23:40:00,952 BAD EPOCHS (no improvement): 1\n",
      "2019-11-23 23:40:04,869 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:40:04,901 Testing using best model ...\n",
      "2019-11-23 23:40:05,046 loading file test/title10/best-model.pt\n",
      "2019-11-23 23:40:12,514 0.8852\t0.8852\t0.8852\n",
      "2019-11-23 23:40:12,515 \n",
      "MICRO_AVG: acc 0.7941 - f1-score 0.8852\n",
      "MACRO_AVG: acc 0.794 - f1-score 0.88515\n",
      "fake       tp: 26 - fp: 5 - fn: 2 - tn: 28 - precision: 0.8387 - recall: 0.9286 - accuracy: 0.7879 - f1-score: 0.8814\n",
      "real       tp: 28 - fp: 2 - fn: 5 - tn: 26 - precision: 0.9333 - recall: 0.8485 - accuracy: 0.8000 - f1-score: 0.8889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:40:12,517 ----------------------------------------------------------------------------------------------------\n",
      "2019-11-23 23:40:12,522 loading file ./test/title10/best-model.pt\n",
      "acc:  0.7435897435897436\n",
      "precision:  0.6907216494845361\n",
      "recall:  0.8701298701298701\n",
      "f1:  0.7701149425287357\n"
     ]
    }
   ],
   "source": [
    "path = './test/title10'\n",
    "make_test(train_title, test_title, path, path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0.691\n",
    "precision = 0.638\n",
    "recall = 0.789\n",
    "f1 = 0.706\n",
    "\n",
    "acc2 = 0.7435897435897436\n",
    "precision2 = 0.6907216494845361\n",
    "recall2 = 0.8701298701298701\n",
    "f12 = 0.7701149425287357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(old, new, name=''):\n",
    "    print(name, (new-old)*100, (new/old-1)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 5.258974358974367 7.610672010093156\n",
      "precision 5.2721649484536055 8.263581423908484\n",
      "recall 8.112987012987006 10.28261978832321\n",
      "f1 6.411494252873573 9.081436618801098\n"
     ]
    }
   ],
   "source": [
    "diff(acc, acc2, 'acc')\n",
    "diff(precision, precision2, 'precision')\n",
    "diff(recall, recall2, 'recall')\n",
    "diff(f1, f12, 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', 'text', 'images', 'top_img', 'keywords', 'authors', 'canonical_link', 'title', 'meta_data', 'movies', 'publish_date', 'source', 'summary'])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643\n",
      "162\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_summary = [(x['summary'], label) for x, label in train_content] \n",
    "test_summary = [(x['summary'], label) for x, label in test_content]\n",
    "print(len([x for x, label in train_summary if x == '']))\n",
    "print(len([x for x, label in test_summary if x == '']))\n",
    "train_summary = [(x, label) for x, label in train_summary if x != ''] \n",
    "test_summary = [(x, label) for x, label in test_summary if x != '']\n",
    "print(len(train_summary))\n",
    "print(len(test_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "11\n",
      "581\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "train_text = [(x['text'], label) for x, label in train_content] \n",
    "test_text = [(x['text'], label) for x, label in test_content]\n",
    "print(len([x for x, label in train_text if x == '']))\n",
    "print(len([x for x, label in test_text if x == '']))\n",
    "train_text = [(x, label) for x, label in train_text if x != ''] \n",
    "test_text = [(x, label) for x, label in test_text if x != '']\n",
    "print(len(train_text))\n",
    "print(len(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-23 23:58:03,519 Reading data from test/text1\n",
      "2019-11-23 23:58:03,521 Train: test/text1/train.csv\n",
      "2019-11-23 23:58:03,522 Dev: test/text1/dev.csv\n",
      "2019-11-23 23:58:03,523 Test: test/text1/test.csv\n",
      "2019-11-23 23:58:03,585 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/464 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 14/464 [00:00<00:08, 51.15it/s]\u001b[A\n",
      "  7%|▋         | 31/464 [00:00<00:07, 61.81it/s]\u001b[A\n",
      "  8%|▊         | 36/464 [00:00<00:08, 52.63it/s]\u001b[A\n",
      "  8%|▊         | 36/464 [03:20<00:08, 52.63it/s]\u001b[A"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 8377) is killed by signal: Killed. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-b583d6a2f93a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./test/text1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmake_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-781f42cfb8f2>\u001b[0m in \u001b[0;36mmake_test\u001b[0;34m(train_data, test_data, data_folder, model_folder, max_epochs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-781f42cfb8f2>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_data, data_folder, model_folder, max_epochs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0msave_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-781f42cfb8f2>\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(corpus, model_folder, max_epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mlabel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_label_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     word_embeddings = [\n",
      "\u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.7/site-packages/flair/data.py\u001b[0m in \u001b[0;36mmake_label_dictionary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Computing label dictionary. Progress:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 8377) is killed by signal: Killed. "
     ]
    }
   ],
   "source": [
    "path = './test/text1'\n",
    "make_test(train_text, test_text, path, path, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
